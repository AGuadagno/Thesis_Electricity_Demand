{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing libraries\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport seaborn as sns\nimport itertools as it\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\nfrom scipy.stats import multivariate_normal\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\nplt.rc('font', size=16)\n\nimport warnings\nwarnings.filterwarnings('ignore')\ntf.get_logger().setLevel('ERROR')\n\ntfk = tf.keras\ntfkl = tf.keras.layers\n\n# Setting random seed for reproducibility\n\nseed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\ntfp.random.sanitize_seed(seed)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-09-16T09:06:10.411245Z","iopub.execute_input":"2022-09-16T09:06:10.411867Z","iopub.status.idle":"2022-09-16T09:06:21.358385Z","shell.execute_reply.started":"2022-09-16T09:06:10.411742Z","shell.execute_reply":"2022-09-16T09:06:21.357454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Opening the dataframe\nABB_data = pd.read_csv('../input/electricitydemand/Multivariate_Labeled_2 (1).csv', parse_dates=['DateTime'], delimiter=',')\nABB_data.drop(columns=\"DateTime\", inplace=True)\nABB_data","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:21.360417Z","iopub.execute_input":"2022-09-16T09:06:21.361218Z","iopub.status.idle":"2022-09-16T09:06:21.590663Z","shell.execute_reply.started":"2022-09-16T09:06:21.361179Z","shell.execute_reply":"2022-09-16T09:06:21.589573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataframe preprocessing\n\n# Converting data types\nABB_data['CB_First_Floor'] = ABB_data['CB_First_Floor'].astype(np.float32)\nABB_data['CB_Second_Floor'] = ABB_data['CB_Second_Floor'].astype(np.float32)\nABB_data['AvgT'] = ABB_data['AvgT'].astype(np.float32)\nABB_data['AvgH'] = ABB_data['AvgH'].astype(np.float32)\nABB_data['Anomaly_Label'] = ABB_data['Anomaly_Label'].astype(np.float32)\nABB_data['Year'] = ABB_data['Year'].astype(np.float32)\nABB_data['Month'] = ABB_data['Month'].astype(np.float32)\nABB_data['Day'] = ABB_data['Day'].astype(np.float32)\nABB_data['Hour'] = ABB_data['Hour'].astype(np.float32)\nABB_data['Min'] = ABB_data['Min'].astype(np.float32)\nABB_data['Sec'] = ABB_data['Sec'].astype(np.float32)\nABB_data['Day_Of_The_Week'] = ABB_data['Day_Of_The_Week'].astype(np.float32)\n\n# Printing the dataframe's info\nABB_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:21.591995Z","iopub.execute_input":"2022-09-16T09:06:21.592433Z","iopub.status.idle":"2022-09-16T09:06:21.630525Z","shell.execute_reply.started":"2022-09-16T09:06:21.592388Z","shell.execute_reply":"2022-09-16T09:06:21.629329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function used to plot time series\ndef inspect_dataframe(df, columns):\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(15,10))\n    for i, col in enumerate(columns):\n        axs[i].plot(df[col])\n        axs[i].set_title(col)\n    plt.show()\n    \n# Plotting time series\ninspect_dataframe(ABB_data[[\"CB_First_Floor\", \"CB_Second_Floor\", \"Anomaly_Label\", \"Day_Of_The_Week\"]], ABB_data[[\"CB_First_Floor\", \"CB_Second_Floor\", \"Anomaly_Label\", \"Day_Of_The_Week\"]].columns)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:21.633432Z","iopub.execute_input":"2022-09-16T09:06:21.633801Z","iopub.status.idle":"2022-09-16T09:06:22.208902Z","shell.execute_reply.started":"2022-09-16T09:06:21.633766Z","shell.execute_reply":"2022-09-16T09:06:22.207987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns used for the following split in train, test and validation sets\nABB_data['Test'] = 0\nABB_data['Validation'] = 0\nABB_data['Train_P1'] = 0\nABB_data['Train_P2'] = 0","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:22.209959Z","iopub.execute_input":"2022-09-16T09:06:22.210902Z","iopub.status.idle":"2022-09-16T09:06:22.219322Z","shell.execute_reply.started":"2022-09-16T09:06:22.210872Z","shell.execute_reply":"2022-09-16T09:06:22.218186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST SET\n\n# Some random weeks\nweek = [89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111]\nfor j in week:\n    Slice_Multivariate_data = ABB_data.iloc[(j)*672:(j+1)*672]\n    for k in range((j)*672,((j+1)*672)):\n        ABB_data.at[k, 'Test'] = 1\n        \nj=112\nfor k in range((j)*672,((j+1)*672)-96):\n        ABB_data.at[k, 'Test'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:22.221073Z","iopub.execute_input":"2022-09-16T09:06:22.222099Z","iopub.status.idle":"2022-09-16T09:06:22.362069Z","shell.execute_reply.started":"2022-09-16T09:06:22.222058Z","shell.execute_reply":"2022-09-16T09:06:22.361119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VALIDATION SET\n\n# Some random weeks\nweek = [55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78]\nfor j in week:\n    Slice_Multivariate_data = ABB_data.iloc[(j)*672:(j+1)*672]\n    for k in range((j)*672,(j+1)*672):\n        ABB_data.at[k, 'Validation'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:22.363543Z","iopub.execute_input":"2022-09-16T09:06:22.363987Z","iopub.status.idle":"2022-09-16T09:06:22.502917Z","shell.execute_reply.started":"2022-09-16T09:06:22.363951Z","shell.execute_reply":"2022-09-16T09:06:22.501952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAINING SET\n\nweek = [x for x in np.arange(0, 55, 1)]\nfor j in week:\n    Slice_Univariate_data = ABB_data.iloc[(j)*672:(j+1)*672]\n    for k in range((j)*672,(j+1)*672):\n        ABB_data.at[k, 'Train_P1'] = 1\n    \nweek = [x for x in np.arange(79, 89, 1)]\nfor j in week:\n    Slice_Univariate_data = ABB_data.iloc[(j)*672:(j+1)*672]\n    for k in range((j)*672,(j+1)*672):\n        ABB_data.at[k, 'Train_P2'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:22.504370Z","iopub.execute_input":"2022-09-16T09:06:22.504932Z","iopub.status.idle":"2022-09-16T09:06:22.868782Z","shell.execute_reply.started":"2022-09-16T09:06:22.504897Z","shell.execute_reply":"2022-09-16T09:06:22.867749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting time series\ninspect_dataframe(ABB_data[[\"CB_First_Floor\", \"CB_Second_Floor\", \"Anomaly_Label\", \"Day_Of_The_Week\", \"Train_P1\", \"Train_P2\", \"Validation\", \"Test\"]], ABB_data[[\"CB_First_Floor\", \"CB_Second_Floor\", \"Anomaly_Label\", \"Day_Of_The_Week\", \"Train_P1\", \"Train_P2\", \"Validation\", \"Test\"]].columns)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:22.870206Z","iopub.execute_input":"2022-09-16T09:06:22.870573Z","iopub.status.idle":"2022-09-16T09:06:23.710195Z","shell.execute_reply.started":"2022-09-16T09:06:22.870529Z","shell.execute_reply":"2022-09-16T09:06:23.709286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN, TEST, VALIDATION SETS\nX_train_temp_P1 = ABB_data[ABB_data['Train_P1']==1].copy()\nX_train_temp_P2 = ABB_data[ABB_data['Train_P2']==1].copy()\nX_validation_temp = ABB_data[ABB_data['Validation']==1].copy()\nX_test_temp = ABB_data[ABB_data['Test']==1].copy()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:23.714500Z","iopub.execute_input":"2022-09-16T09:06:23.715438Z","iopub.status.idle":"2022-09-16T09:06:23.731305Z","shell.execute_reply.started":"2022-09-16T09:06:23.715399Z","shell.execute_reply":"2022-09-16T09:06:23.730458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training, Validation and Test set - Anomalies (not used for training)\nAnomaly_Train_P1 = X_train_temp_P1[['Anomaly_Label']]\nAnomaly_Train_P2 = X_train_temp_P2[['Anomaly_Label']]\nAnomaly_Validation = X_validation_temp[['Anomaly_Label']]\nAnomaly_Test = X_test_temp[['Anomaly_Label']]","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:23.732602Z","iopub.execute_input":"2022-09-16T09:06:23.733022Z","iopub.status.idle":"2022-09-16T09:06:23.740940Z","shell.execute_reply.started":"2022-09-16T09:06:23.732987Z","shell.execute_reply":"2022-09-16T09:06:23.739879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Anomaly_Test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:23.742271Z","iopub.execute_input":"2022-09-16T09:06:23.743001Z","iopub.status.idle":"2022-09-16T09:06:23.755664Z","shell.execute_reply.started":"2022-09-16T09:06:23.742960Z","shell.execute_reply":"2022-09-16T09:06:23.754586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_temp_global = pd.concat([X_train_temp_P1,X_train_temp_P2],axis=0)\nX_train_temp_global.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:23.757183Z","iopub.execute_input":"2022-09-16T09:06:23.757532Z","iopub.status.idle":"2022-09-16T09:06:23.949951Z","shell.execute_reply.started":"2022-09-16T09:06:23.757499Z","shell.execute_reply":"2022-09-16T09:06:23.948607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# STANDARDIZATION\n\nmean = X_train_temp_global.mean()\nstd_dev = X_train_temp_global.std()\nprint(mean, std_dev)\n\nX_train_raw_P1 = (X_train_temp_P1-mean)/std_dev\nX_train_raw_P2 = (X_train_temp_P2-mean)/std_dev\nX_validation_raw = (X_validation_temp-mean)/std_dev\nX_test_raw = (X_test_temp-mean)/std_dev\n\nprint(X_test_raw.shape[0]+X_validation_raw.shape[0]+X_train_raw_P1.shape[0]+X_train_raw_P2.shape[0] == ABB_data.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:23.952046Z","iopub.execute_input":"2022-09-16T09:06:23.952826Z","iopub.status.idle":"2022-09-16T09:06:23.979281Z","shell.execute_reply.started":"2022-09-16T09:06:23.952778Z","shell.execute_reply":"2022-09-16T09:06:23.978235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Anomalies are not considered during the training\nX_train_raw_P1 = X_train_raw_P1[['CB_First_Floor', 'CB_Second_Floor', 'Day_Of_The_Week']]\nX_train_raw_P2 = X_train_raw_P2[['CB_First_Floor', 'CB_Second_Floor', 'Day_Of_The_Week']]\nX_validation_raw = X_validation_raw[['CB_First_Floor', 'CB_Second_Floor', 'Day_Of_The_Week']]\nX_test_raw = X_test_raw[['CB_First_Floor', 'CB_Second_Floor', 'Day_Of_The_Week']]\nX_train_raw_P1.shape, X_train_raw_P2.shape, X_validation_raw.shape, X_test_raw.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:23.980519Z","iopub.execute_input":"2022-09-16T09:06:23.980986Z","iopub.status.idle":"2022-09-16T09:06:23.997828Z","shell.execute_reply.started":"2022-09-16T09:06:23.980950Z","shell.execute_reply":"2022-09-16T09:06:23.996949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting time series\ninspect_dataframe(X_test_raw, X_test_raw.columns)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:23.999343Z","iopub.execute_input":"2022-09-16T09:06:23.999714Z","iopub.status.idle":"2022-09-16T09:06:24.416463Z","shell.execute_reply.started":"2022-09-16T09:06:23.999678Z","shell.execute_reply":"2022-09-16T09:06:24.415535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Preparation","metadata":{}},{"cell_type":"code","source":"seed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\ntfp.random.sanitize_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.417738Z","iopub.execute_input":"2022-09-16T09:06:24.418703Z","iopub.status.idle":"2022-09-16T09:06:24.428572Z","shell.execute_reply.started":"2022-09-16T09:06:24.418664Z","shell.execute_reply":"2022-09-16T09:06:24.427520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training parameters\n\nwindow = 672       # 1 week\nstride = 8         # 1 hour\nlatent_dim = 10    # Latent dimension\nepochs = 500       # Number of epochs (no early stopping)\nbatch_size = 8     # Batch size\nalpha = 0.5\nM = 100\nnoise_factor = [0.2, 0.2, 0.2]","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.430090Z","iopub.execute_input":"2022-09-16T09:06:24.431119Z","iopub.status.idle":"2022-09-16T09:06:24.439371Z","shell.execute_reply.started":"2022-09-16T09:06:24.431077Z","shell.execute_reply":"2022-09-16T09:06:24.438482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function used to create (overlapped) windows that we will use for the training\ndef build_sequences(df, anomaly, window=window, stride=stride):\n    \n    assert len(df) > window\n    dataset = []\n    anomalies = []\n    temp_df = df.copy().values\n    temp_an = anomaly.copy().values\n    padding_len = (len(df)-window)%stride # Padding computed considering the stride\n    print(\"PADDING: \" + str(padding_len)) #DEBUG\n\n    if(padding_len != 0):\n        # Compute padding length\n        padding_len = window - len(df)%window\n        padding = np.zeros((padding_len,temp_df.shape[1]), dtype='float32')\n        temp_df = np.concatenate((padding,df))\n        padding = np.zeros((padding_len,temp_an.shape[1]), dtype='float32')\n        temp_an = np.concatenate((padding,temp_an))\n        assert len(temp_df) % window == 0\n\n    for idx in np.arange(0,len(temp_df)-window+1,stride):\n        dataset.append(temp_df[idx:idx+window])\n        anomalies.append(temp_an[idx:idx+window])\n\n    dataset = np.array(dataset)\n    anomalies = np.array(anomalies)\n    return dataset, anomalies","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.440501Z","iopub.execute_input":"2022-09-16T09:06:24.441438Z","iopub.status.idle":"2022-09-16T09:06:24.451937Z","shell.execute_reply.started":"2022-09-16T09:06:24.441401Z","shell.execute_reply":"2022-09-16T09:06:24.450682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_P1, A_train_P1 = build_sequences(X_train_raw_P1, Anomaly_Train_P1, window=window, stride=stride)\nX_train_P2, A_train_P2 = build_sequences(X_train_raw_P2, Anomaly_Train_P2, window=window, stride=stride)\nX_val, A_val = build_sequences(X_validation_raw, Anomaly_Validation, window=window, stride=stride)\nX_train_P1.shape,X_train_P2.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.453434Z","iopub.execute_input":"2022-09-16T09:06:24.453832Z","iopub.status.idle":"2022-09-16T09:06:24.552668Z","shell.execute_reply.started":"2022-09-16T09:06:24.453797Z","shell.execute_reply":"2022-09-16T09:06:24.551432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Temp test set - plotting purposes\nX_test, A_test = build_sequences(X_test_raw, Anomaly_Test, window=window, stride=stride)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.554284Z","iopub.execute_input":"2022-09-16T09:06:24.554755Z","iopub.status.idle":"2022-09-16T09:06:24.586489Z","shell.execute_reply.started":"2022-09-16T09:06:24.554717Z","shell.execute_reply":"2022-09-16T09:06:24.585448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.concatenate([X_train_P1,X_train_P2],axis=0)\nA_train = np.concatenate([A_train_P1,A_train_P2],axis=0)\nX_train.shape, A_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.588055Z","iopub.execute_input":"2022-09-16T09:06:24.588431Z","iopub.status.idle":"2022-09-16T09:06:24.633416Z","shell.execute_reply.started":"2022-09-16T09:06:24.588394Z","shell.execute_reply":"2022-09-16T09:06:24.632242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the model","metadata":{}},{"cell_type":"code","source":"seed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.635180Z","iopub.execute_input":"2022-09-16T09:06:24.635573Z","iopub.status.idle":"2022-09-16T09:06:24.641944Z","shell.execute_reply.started":"2022-09-16T09:06:24.635530Z","shell.execute_reply":"2022-09-16T09:06:24.640648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reparametrization trick\ndef sample_z1(args):\n    z_mean, z_log_var = args\n    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1]))\n    return z_mean + tf.exp(alpha * z_log_var) * eps\n\n# Reparametrization trick\ndef sample_z2(args):\n    z_mean, z_log_var = args\n    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n    return z_mean + tf.exp(alpha * z_log_var) * eps","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.643683Z","iopub.execute_input":"2022-09-16T09:06:24.644821Z","iopub.status.idle":"2022-09-16T09:06:24.652441Z","shell.execute_reply.started":"2022-09-16T09:06:24.644617Z","shell.execute_reply":"2022-09-16T09:06:24.651356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AddNoise(tfk.layers.Layer):\n    \n    def call(self, inputs):\n        data, noise_factor = inputs\n        noise = tf.map_fn(fn=lambda t: tf.random.normal((672,3), 0, 1)*noise_factor, elems=data)\n        noise_input = data + noise\n        \n        return noise_input","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.655276Z","iopub.execute_input":"2022-09-16T09:06:24.656071Z","iopub.status.idle":"2022-09-16T09:06:24.662542Z","shell.execute_reply.started":"2022-09-16T09:06:24.656037Z","shell.execute_reply":"2022-09-16T09:06:24.661770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimScore(tfk.layers.Layer):\n\n    def call(self, inputs):\n        seq, h_dim = inputs  # seq: batch x x_dim x h_dim*2\n\n        S = tf.map_fn(fn=lambda t: tf.linalg.matmul(tf.transpose(t), t), elems=seq) # batch x h_dim x h_dim\n        S = tf.map_fn(fn=lambda t: t / tf.math.sqrt((tf.cast(h_dim*2, dtype=tf.float32))), elems=S)\n        A = tf.map_fn(fn=lambda t: tf.keras.activations.softmax(t), elems=S)\n        C = tf.matmul(seq, A)\n\n        return C","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.663825Z","iopub.execute_input":"2022-09-16T09:06:24.664822Z","iopub.status.idle":"2022-09-16T09:06:24.673167Z","shell.execute_reply.started":"2022-09-16T09:06:24.664788Z","shell.execute_reply":"2022-09-16T09:06:24.672073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\n\nfrom keras import backend as K\nfrom tensorflow.keras import Input\nattention_dim = 10\n\ninput_shape = X_train.shape[1:]\noutput_shape = X_train.shape[1:]\n\n###########\n# ENCODER #\n###########\n\nencoder_input = tf.keras.Input(shape=input_shape)\n\nnoisy_input = AddNoise()([encoder_input, noise_factor])\n\nh_seq, forward_h, forward_c, backward_h, backward_c = tfkl.Bidirectional(tfkl.LSTM(72, activation='tanh', return_sequences=True, return_state=True))(noisy_input)\n\nstate_h = tfkl.Concatenate()([forward_h, backward_h])\n\nCdet = SimScore()([h_seq, window])\n\nc_mean = tfkl.Dense(attention_dim , activation='linear', name=\"c_mean\")(Cdet)\nc_log_var = tfkl.Dense(attention_dim , activation='softplus', name=\"c_var\")(Cdet)\n\n# Latent representation: mean + log of std.dev.\nz_mean = tfkl.Dense(latent_dim, activation='linear', name=\"z_mean\")(state_h)\nz_log_var = tfkl.Dense(latent_dim, activation='softplus', name=\"z_var\")(state_h)\n\n# Sampling a vector from the latent distribution\nz = tfkl.Lambda(sample_z1, name='z')([z_mean, z_log_var])\nc = tfkl.Lambda(sample_z2, name='c')([c_mean, c_log_var])\n\nencoder = tfk.Model(encoder_input, [z_mean, z_log_var, z, c_mean, c_log_var, c], name='encoder')\nprint(encoder.summary())","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:24.674670Z","iopub.execute_input":"2022-09-16T09:06:24.675064Z","iopub.status.idle":"2022-09-16T09:06:25.468916Z","shell.execute_reply.started":"2022-09-16T09:06:24.675024Z","shell.execute_reply":"2022-09-16T09:06:25.468022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###########\n# DECODER #\n###########\n\nz_inputs = Input(shape=(latent_dim, ), name='decoder_input_1')\nc_inputs = Input(shape=(window, attention_dim), name='decoder_input_2')\n\nrepeated = tfkl.RepeatVector(window)(z_inputs)\nconcat = tfkl.Concatenate(axis=-1)([repeated, c_inputs])\n\nx = tfkl.Bidirectional(tfkl.LSTM(72, return_sequences=True))(concat)\n\n# OPZIONE 1 - Dense Layers come nel paper\nmu = tfkl.Dense(3, activation='linear', name=\"mu\")(x)\nlog_sigma = tfkl.Dense(3, activation='softplus', name=\"sigma\")(x)\n\n# Define and summarize decoder model\ndecoder = tfk.Model([z_inputs, c_inputs], [mu, log_sigma], name='decoder')\ndecoder.summary()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:25.475312Z","iopub.execute_input":"2022-09-16T09:06:25.475601Z","iopub.status.idle":"2022-09-16T09:06:25.914336Z","shell.execute_reply.started":"2022-09-16T09:06:25.475574Z","shell.execute_reply":"2022-09-16T09:06:25.913440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VAE(tfk.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n        self.likelihood_tracker = tfk.metrics.Mean(name=\"likelihood\")\n        self.kl_loss_z_tracker = tfk.metrics.Mean(name=\"kl_loss_z\")\n        self.kl_loss_c_tracker = tfk.metrics.Mean(name=\"kl_loss_c\")\n        self.reconstruction_loss_tracker = tfk.metrics.Mean(name=\"reconstruction_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.likelihood_tracker,\n            self.kl_loss_z_tracker,\n            self.kl_loss_c_tracker,\n            self.reconstruction_loss_tracker\n        ]\n    \n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            \n            sample = None\n            encoder_mu, encoder_log_var, z, c_mean, c_log_var, c = self.encoder(data)\n            decoder_mu, decoder_log_sigma = self.decoder([z,c])\n            decoder_sigma = tf.exp(decoder_log_sigma)\n            \n            pdf_laplace = tfp.distributions.Laplace(decoder_mu, decoder_sigma, validate_args=True, name='Laplace')\n                        \n            for _ in range(M):\n                sample = pdf_laplace.sample() if sample is None else sample + pdf_laplace.sample()\n                             \n            likelihood = -(pdf_laplace.log_prob(data))\n            likelihood = tf.reduce_mean(likelihood, axis=-1)\n            likelihood = tf.reduce_mean(likelihood, axis=-1)\n                \n            decoder_output = sample/M\n            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, decoder_output), axis=1))\n            \n            kl_loss_z = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n            kl_loss_z = tf.reduce_mean(tf.reduce_sum(kl_loss_z, axis=1))\n            \n            kl_loss_c = -0.5 * (1 + c_log_var - tf.square(c_mean) - tf.exp(c_log_var))\n            kl_loss_c = tf.keras.backend.sum(kl_loss_c, axis=1)\n            kl_loss_c = tf.reduce_mean(kl_loss_c, axis=1)\n\n            total_loss = 4*likelihood + 2*(kl_loss_z + 7*kl_loss_c) + reconstruction_loss\n            \n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.likelihood_tracker.update_state(likelihood)\n        self.kl_loss_z_tracker.update_state(kl_loss_z)\n        self.kl_loss_c_tracker.update_state(kl_loss_c)\n        \n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"likelihood\": self.likelihood_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result()\n        }\n    \n    \n    \n    def test_step(self, data): # https://github.com/keras-team/keras-io/issues/38\n\n        sample = None\n        encoder_mu, encoder_log_var, z, c_mean, c_log_var, c = self.encoder(data)\n        decoder_mu, decoder_log_sigma = self.decoder([z,c])\n        decoder_sigma = tf.exp(decoder_log_sigma)\n            \n        pdf_laplace = tfp.distributions.Laplace(decoder_mu, decoder_sigma, validate_args=True, name='Laplace')\n                        \n        for _ in range(M):\n            sample = pdf_laplace.sample() if sample is None else sample + pdf_laplace.sample()\n                             \n        likelihood = -(pdf_laplace.log_prob(data))\n        likelihood = tf.reduce_mean(likelihood, axis=-1)\n        likelihood = tf.reduce_mean(likelihood, axis=-1)\n                \n        decoder_output = sample/M\n        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, decoder_output), axis=1))\n            \n        kl_loss_z = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n        kl_loss_z = tf.reduce_mean(tf.reduce_sum(kl_loss_z, axis=1))\n            \n        kl_loss_c = -0.5 * (1 + c_log_var - tf.square(c_mean) - tf.exp(c_log_var))\n        kl_loss_c = tf.keras.backend.sum(kl_loss_c, axis=1)\n        kl_loss_c = tf.reduce_mean(kl_loss_c, axis=1)\n\n        total_loss = 4*likelihood + 2*(kl_loss_z + 7*kl_loss_c) + reconstruction_loss\n            \n        self.total_loss_tracker.update_state(total_loss)\n        self.likelihood_tracker.update_state(likelihood)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_z_tracker.update_state(kl_loss_z)\n        self.kl_loss_c_tracker.update_state(kl_loss_c)\n        \n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"likelihood\": self.likelihood_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_c\": self.kl_loss_c_tracker.result(),\n            \"kl_z\": self.kl_loss_z_tracker.result(),\n        }","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:25.915873Z","iopub.execute_input":"2022-09-16T09:06:25.916218Z","iopub.status.idle":"2022-09-16T09:06:25.942074Z","shell.execute_reply.started":"2022-09-16T09:06:25.916184Z","shell.execute_reply":"2022-09-16T09:06:25.940504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae = VAE(encoder, decoder)\n\nseed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\n\nvae.compile(optimizer=tfk.optimizers.Adam())\n\nvae.fit(x = X_train,\n        validation_data = (X_val, None),\n        epochs=epochs, \n        batch_size=batch_size,\n        callbacks=[tfk.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True), tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-5)]\n       )","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:06:25.944000Z","iopub.execute_input":"2022-09-16T09:06:25.944520Z","iopub.status.idle":"2022-09-16T09:51:19.550737Z","shell.execute_reply.started":"2022-09-16T09:06:25.944476Z","shell.execute_reply":"2022-09-16T09:51:19.549705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example of prediction on the test set","metadata":{}},{"cell_type":"code","source":"# Check - Variances on train set\nw = X_train[0,:,:]\nencoder_mu, encoder_log_var, z, c_mean, c_log_var, c = encoder(np.expand_dims(w, axis=0))\nprint(tf.exp(c_log_var))\ndecoder_mu, decoder_log_sigma = decoder([z,c])\nprint(tf.exp(decoder_log_sigma))","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:51:19.552615Z","iopub.execute_input":"2022-09-16T09:51:19.553037Z","iopub.status.idle":"2022-09-16T09:51:19.697449Z","shell.execute_reply.started":"2022-09-16T09:51:19.552999Z","shell.execute_reply":"2022-09-16T09:51:19.696485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inspect_multivariate_prediction(X, y, a, pred, columns, telescope, idx=None):\n\n    figs, axs = plt.subplots(len(columns) + 1, 1, sharex=True, figsize=(17,10))\n    for i, col in enumerate(columns):\n        axs[i].plot(np.arange(len(X[0,:,i])), X[0,:,i])\n        axs[i].plot(np.arange(len(X[0,:,i]), len(X[0,:,i])+telescope), y[0,:,i], color='orange')\n        axs[i].plot(np.arange(len(X[0,:,i]), len(X[0,:,i])+telescope), pred[0,:,i], color='green')\n        axs[i].set_title(col)\n           \n    axs[len(columns)].plot(np.arange(len(X[0,:,i])), a[:,0])\n    axs[len(columns)].plot(np.arange(len(X[0,:,i]), len(X[0,:,i])+telescope), a[:,0])\n    axs[len(columns)].set_title('Anomaly')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:51:19.699031Z","iopub.execute_input":"2022-09-16T09:51:19.699391Z","iopub.status.idle":"2022-09-16T09:51:19.708580Z","shell.execute_reply.started":"2022-09-16T09:51:19.699355Z","shell.execute_reply":"2022-09-16T09:51:19.707523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of prediction on the test set\n\nx = 0\n\nw = X_test[x,:,:] \nw = tf.expand_dims(w, 0)\n\nsample = None\nencoder_mu, encoder_log_var, z, c_mean, c_log_var, c = encoder(w)\ndecoder_mu, decoder_log_sigma = decoder([z,c])\ndecoder_sigma = tf.exp(decoder_log_sigma)\n\na = A_test[x,:]         \npdf_laplace = tfp.distributions.Laplace(decoder_mu, decoder_sigma, validate_args=True, name='Laplace')\n\nfor _ in range(M):\n    sample = pdf_laplace.sample() if sample is None else sample + pdf_laplace.sample()\nreconstruction = sample/M\n\n# Plotting predictions on the test set\ntarget_labels = X_test_raw.columns\ninspect_multivariate_prediction(w, w, a, reconstruction, target_labels, window)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:51:19.710029Z","iopub.execute_input":"2022-09-16T09:51:19.710616Z","iopub.status.idle":"2022-09-16T09:51:20.896411Z","shell.execute_reply.started":"2022-09-16T09:51:19.710579Z","shell.execute_reply":"2022-09-16T09:51:20.895561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of prediction on the test set\n\nx = 84\n\nw = X_test[x,:,:] \nw = tf.expand_dims(w, 0)\n\nsample = None\nencoder_mu, encoder_log_var, z, c_mean, c_log_var, c = encoder(w)\ndecoder_mu, decoder_log_sigma = decoder([z,c])\ndecoder_sigma = tf.exp(decoder_log_sigma)\n\na = A_test[x,:]         \npdf_laplace = tfp.distributions.Laplace(decoder_mu, decoder_sigma, validate_args=True, name='Laplace')\n\nfor _ in range(M):\n    sample = pdf_laplace.sample() if sample is None else sample + pdf_laplace.sample()\nreconstruction = sample/M\n\n# Plotting predictions on the test set\ntarget_labels = X_test_raw.columns\ninspect_multivariate_prediction(w, w, a, reconstruction, target_labels, window)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:51:20.898217Z","iopub.execute_input":"2022-09-16T09:51:20.899284Z","iopub.status.idle":"2022-09-16T09:51:22.093513Z","shell.execute_reply.started":"2022-09-16T09:51:20.899245Z","shell.execute_reply":"2022-09-16T09:51:22.092570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Computing reconstruction probability","metadata":{}},{"cell_type":"code","source":"# Function used to create non-overlapped windows starting from the TRAINING set\ndef build_sequences_reconstruction(df, window, stride):\n    dataset = []\n    temp_df = df.copy().values\n    padding_len = (len(df)-window)%stride\n\n    for idx in np.arange(0,len(temp_df)-window+1,stride):\n        dataset.append(temp_df[idx:idx+window])\n\n    dataset = np.array(dataset)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:51:22.094905Z","iopub.execute_input":"2022-09-16T09:51:22.095607Z","iopub.status.idle":"2022-09-16T09:51:22.103457Z","shell.execute_reply.started":"2022-09-16T09:51:22.095568Z","shell.execute_reply":"2022-09-16T09:51:22.102164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_overlapped = build_sequences_reconstruction(X_test_raw, window=window, stride=stride)\nX_test_overlapped.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:51:22.105256Z","iopub.execute_input":"2022-09-16T09:51:22.105691Z","iopub.status.idle":"2022-09-16T09:51:22.137727Z","shell.execute_reply.started":"2022-09-16T09:51:22.105655Z","shell.execute_reply":"2022-09-16T09:51:22.136647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconstructed_prob_1 = np.zeros(X_test_raw.shape[0], dtype='float32') # Array that will contain rec prob\nreconstructed_prob_2 = np.zeros(X_test_raw.shape[0], dtype='float32') # Array that will contain rec prob\nreconstructed_prob_written_1 = np.zeros(X_test_raw.shape[0], dtype='float32') # Array that will contain rec prob\nreconstructed_prob_written_2 = np.zeros(X_test_raw.shape[0], dtype='float32') # Array that will contain rec prob\nM=25\n\nfor j in range (X_test_overlapped.shape[0]): # Consider all the windows in X_test_overlapped one at a time ...\n    \n    if(j%100==0): print(j)\n    \n    w = X_test_overlapped[j,:,:] \n    w = tf.expand_dims(w, 0)\n    \n    z_mean, z_log_var, z, c_mean, c_log_var, c = encoder(w) # Sampling only 1 time in the latent space\n    \n    if (j == 0):\n        \n        for m in range(M):\n            \n            decoder_mu, decoder_log_sigma = decoder([z,c])\n            decoder_sigma = tf.exp(decoder_log_sigma)\n            \n            for i in range(window):  \n                pdf_laplace_1 = tfp.distributions.Laplace(decoder_mu[0, i, 0], decoder_sigma[0, i, 0], validate_args=True, name='Laplace1')\n                pdf_laplace_2 = tfp.distributions.Laplace(decoder_mu[0, i, 1], decoder_sigma[0, i, 1], validate_args=True, name='Laplace2')\n                p_l_1 = -(pdf_laplace_1.log_prob(X_test_overlapped[j, i, 0])) \n                p_l_2 = -(pdf_laplace_2.log_prob(X_test_overlapped[j, i, 1])) \n                reconstructed_prob_1[i] = reconstructed_prob_1[i] + p_l_1\n                reconstructed_prob_2[i] = reconstructed_prob_2[i] + p_l_2\n                reconstructed_prob_written_1[i] = reconstructed_prob_written_1[i] + 1\n                reconstructed_prob_written_2[i] = reconstructed_prob_written_2[i] + 1\n                \n    else:\n        \n        for m in range(M):\n            \n            decoder_mu, decoder_log_sigma = decoder([z,c])\n            decoder_sigma = tf.exp(decoder_log_sigma)\n            \n            for i in range(stride):\n                pdf_laplace_1 = tfp.distributions.Laplace(decoder_mu[0, window-stride+i, 0], decoder_sigma[0, window-stride+i, 0], validate_args=True)\n                pdf_laplace_2 = tfp.distributions.Laplace(decoder_mu[0, window-stride+i, 1], decoder_sigma[0, window-stride+i, 1], validate_args=True)\n                p_l_1 = -(pdf_laplace_1.log_prob(X_test_overlapped[j, window-stride+i, 0]))   \n                p_l_2 = -(pdf_laplace_2.log_prob(X_test_overlapped[j, window-stride+i, 1])) \n                if window + (j-1)*stride + i < X_test_raw.shape[0]: # To avoid problems related to padding\n                    reconstructed_prob_1[window + (j-1) * stride + i] = reconstructed_prob_1[window + (j-1) * stride + i] + p_l_1\n                    reconstructed_prob_2[window + (j-1) * stride + i] = reconstructed_prob_2[window + (j-1) * stride + i] + p_l_2\n                    reconstructed_prob_written_1[window + (j-1) * stride + i] = reconstructed_prob_written_1[window + (j-1) * stride + i] + 1\n                    reconstructed_prob_written_2[window + (j-1) * stride + i] = reconstructed_prob_written_2[window + (j-1) * stride + i] + 1\n                    \n        \nreconstructed_prob_1 /= M\nreconstructed_prob_2 /= M\nreconstructed_prob_1.shape, reconstructed_prob_2.shape, reconstructed_prob_1.max(), reconstructed_prob_1.min(), reconstructed_prob_2.max(), reconstructed_prob_2.min()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T09:51:22.139204Z","iopub.execute_input":"2022-09-16T09:51:22.139553Z","iopub.status.idle":"2022-09-16T10:00:46.084697Z","shell.execute_reply.started":"2022-09-16T09:51:22.139518Z","shell.execute_reply":"2022-09-16T10:00:46.083658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconstructed_prob_written_1.sum(), reconstructed_prob_written_2.sum(), X_test_raw.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:00:46.086244Z","iopub.execute_input":"2022-09-16T10:00:46.086657Z","iopub.status.idle":"2022-09-16T10:00:46.094729Z","shell.execute_reply.started":"2022-09-16T10:00:46.086603Z","shell.execute_reply":"2022-09-16T10:00:46.093573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking if we've written correct things inside \"reconstruction_prob\"\ncheck = True\nfor i in range(reconstructed_prob_written_1.shape[0]):\n    if reconstructed_prob_written_1[i] != M: # Each cell written M times\n        check = False\n        \nprint(check) # If true, everything okay","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:11.013468Z","iopub.execute_input":"2022-09-16T10:01:11.014449Z","iopub.status.idle":"2022-09-16T10:01:11.053891Z","shell.execute_reply.started":"2022-09-16T10:01:11.014399Z","shell.execute_reply":"2022-09-16T10:01:11.052819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counting the number of elements in \"reconstructed_prob\" that are equal to 0\ncounter = 0\nfor i in range(reconstructed_prob_1.shape[0]):\n    if reconstructed_prob_1[i] == 0:\n        counter += 1\n        \nprint(counter)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:15.832619Z","iopub.execute_input":"2022-09-16T10:01:15.833125Z","iopub.status.idle":"2022-09-16T10:01:15.895537Z","shell.execute_reply.started":"2022-09-16T10:01:15.833086Z","shell.execute_reply":"2022-09-16T10:01:15.894530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconstructed_prob = reconstructed_prob_1 + reconstructed_prob_2","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:20.588031Z","iopub.execute_input":"2022-09-16T10:01:20.588545Z","iopub.status.idle":"2022-09-16T10:01:20.597722Z","shell.execute_reply.started":"2022-09-16T10:01:20.588499Z","shell.execute_reply":"2022-09-16T10:01:20.596353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function used to predict anomalies\ndef predict_anomalies(reconstruction_probabilities, threshold):\n    \n    # Inizially we don't have predicted anomalies\n    anomaly_predicted = np.zeros(shape=X_test_raw.shape[0])\n    \n    for i in range(reconstruction_probabilities.shape[0]): \n        if reconstruction_probabilities[i] > threshold:\n            anomaly_predicted[i] = 1\n            \n    return anomaly_predicted","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:22.551923Z","iopub.execute_input":"2022-09-16T10:01:22.552287Z","iopub.status.idle":"2022-09-16T10:01:22.558333Z","shell.execute_reply.started":"2022-09-16T10:01:22.552255Z","shell.execute_reply":"2022-09-16T10:01:22.557036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def select_best_threshold(reconstructed_prob, step, anomaly_true):\n    \n    min_prob = reconstructed_prob.min()\n    max_prob = reconstructed_prob.max()\n    max_F1 = 0\n    best_threshold = 0\n    counter = 0\n    \n    print(\"best threshold = \" + str(best_threshold) + \", max F1 = \" + str(max_F1))\n    \n    thresholds = [x for x in np.arange(min_prob, max_prob, step)]\n    \n    for threshold in thresholds:\n        \n        # print(\"Trying threshold: \" + str(threshold))\n        \n        if(counter<1000):\n            \n            anomaly_predicted = predict_anomalies(reconstructed_prob, threshold)\n            f1 = f1_score(y_true=anomaly_true, y_pred=anomaly_predicted)\n            if f1 > max_F1:\n                max_F1 = f1\n                best_threshold = threshold\n                print(\"best threshold =\" + str(best_threshold) + \", max F1 = \" + str(max_F1))\n            else:\n                counter = counter+1\n        \n        else:\n            return best_threshold","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:06:13.290349Z","iopub.execute_input":"2022-09-16T10:06:13.290744Z","iopub.status.idle":"2022-09-16T10:06:13.299047Z","shell.execute_reply.started":"2022-09-16T10:06:13.290708Z","shell.execute_reply":"2022-09-16T10:06:13.297880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"step = 1/1000\nthreshold = select_best_threshold(reconstructed_prob, step, Anomaly_Test)\nanomaly_predicted = predict_anomalies(reconstructed_prob, threshold)\n\nthreshold, anomaly_predicted.sum(), anomaly_predicted.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:06:14.359918Z","iopub.execute_input":"2022-09-16T10:06:14.360377Z","iopub.status.idle":"2022-09-16T10:07:44.846698Z","shell.execute_reply.started":"2022-09-16T10:06:14.360336Z","shell.execute_reply":"2022-09-16T10:07:44.845569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Anomaly_Test.reset_index(inplace=True)\nAnomaly_Test.drop(columns='index', inplace=True)\nAnomaly_Test","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:48.278551Z","iopub.execute_input":"2022-09-16T10:01:48.279045Z","iopub.status.idle":"2022-09-16T10:01:48.297015Z","shell.execute_reply.started":"2022-09-16T10:01:48.279004Z","shell.execute_reply":"2022-09-16T10:01:48.295692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_raw.reset_index(inplace=True)\nX_test_raw.drop(columns='index', inplace=True)\nX_test_raw","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:50.177062Z","iopub.execute_input":"2022-09-16T10:01:50.177547Z","iopub.status.idle":"2022-09-16T10:01:50.201002Z","shell.execute_reply.started":"2022-09-16T10:01:50.177498Z","shell.execute_reply":"2022-09-16T10:01:50.199692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Anomaly_Test.sum()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:51.949740Z","iopub.execute_input":"2022-09-16T10:01:51.950126Z","iopub.status.idle":"2022-09-16T10:01:51.962348Z","shell.execute_reply.started":"2022-09-16T10:01:51.950092Z","shell.execute_reply":"2022-09-16T10:01:51.961344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_raw['Anomaly_Label_GT'] = Anomaly_Test # Ground truth anomalies\nX_test_raw['Anomaly_Predicted'] = anomaly_predicted \nX_test_raw['Reconstruction_Probability'] = reconstructed_prob\nX_test_raw.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:53.692731Z","iopub.execute_input":"2022-09-16T10:01:53.693238Z","iopub.status.idle":"2022-09-16T10:01:53.722609Z","shell.execute_reply.started":"2022-09-16T10:01:53.693192Z","shell.execute_reply":"2022-09-16T10:01:53.721342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function used to plot time series\ndef inspect_dataframe(df, columns):\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(30,20))\n    for i, col in enumerate(columns):\n        axs[i].plot(df[col])\n        axs[i].set_title(col)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:55.051441Z","iopub.execute_input":"2022-09-16T10:01:55.051995Z","iopub.status.idle":"2022-09-16T10:01:55.067393Z","shell.execute_reply.started":"2022-09-16T10:01:55.051959Z","shell.execute_reply":"2022-09-16T10:01:55.066321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inspect_multivariate_prediction(X, y, a_true, a_predicted, reconstruction_prob, columns): \n   \n    figs, axs = plt.subplots(5, 1, sharex=True, figsize=(17,17)) \n    for i, col in enumerate(columns[:2]): \n        axs[i].plot(np.arange(len(X[:,i])), X[:,i]) \n        axs[i].plot(np.arange(len(X[:,i]), len(X[:,i])*2), y[:,i], color='orange')  \n        axs[i].set_title(col) \n                 \n    axs[2].plot(np.arange(len(X[:,i])), a_true) \n    axs[2].plot(np.arange(len(X[:,i]), len(X[:,i])*2), a_true) \n    axs[2].set_title('Anomaly_Label_GT') \n    \n    axs[3].plot(np.arange(len(X[:,i])), a_predicted) \n    axs[3].plot(np.arange(len(X[:,i]), len(X[:,i])*2), a_predicted) \n    axs[3].set_title('Anomaly_Predicted')\n    \n    axs[4].plot(np.arange(len(X[:,i])), reconstruction_prob) \n    axs[4].plot(np.arange(len(X[:,i]), len(X[:,i])*2), reconstruction_prob) \n    axs[4].set_title('Reconstrution_Probability') \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:56.970609Z","iopub.execute_input":"2022-09-16T10:01:56.971125Z","iopub.status.idle":"2022-09-16T10:01:56.990498Z","shell.execute_reply.started":"2022-09-16T10:01:56.971081Z","shell.execute_reply":"2022-09-16T10:01:56.989221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\n\ni=4\nj=i+3\n\nX = np.array(X_test_raw.iloc[i*672:j*672][['CB_First_Floor', 'CB_Second_Floor']])\nanomaly_true = X_test_raw.iloc[i*672:j*672]['Anomaly_Label_GT'].copy()\nanomaly_prediction = X_test_raw.iloc[i*672:j*672]['Anomaly_Predicted'].copy()\nreconstruction_probability = X_test_raw.iloc[i*672:j*672]['Reconstruction_Probability'].copy()\n\n# Plotting predictionsreconstruction_loss2 = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, y), axis=1))\ninspect_multivariate_prediction(X, X, anomaly_true, anomaly_prediction, reconstruction_probability, target_labels)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:01:58.508449Z","iopub.execute_input":"2022-09-16T10:01:58.508959Z","iopub.status.idle":"2022-09-16T10:01:59.403896Z","shell.execute_reply.started":"2022-09-16T10:01:58.508915Z","shell.execute_reply":"2022-09-16T10:01:59.402840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\n\ni=4\nj=i+1\n\nX = np.array(X_test_raw.iloc[i*672:j*672][['CB_First_Floor', 'CB_Second_Floor']])\nanomaly_true = X_test_raw.iloc[i*672:j*672]['Anomaly_Label_GT'].copy()\nanomaly_prediction = X_test_raw.iloc[i*672:j*672]['Anomaly_Predicted'].copy()\nreconstruction_probability = X_test_raw.iloc[i*672:j*672]['Reconstruction_Probability'].copy()\n\n# Plotting predictionsreconstruction_loss2 = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, y), axis=1))\ninspect_multivariate_prediction(X, X, anomaly_true, anomaly_prediction, reconstruction_probability, target_labels)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:02:08.582586Z","iopub.execute_input":"2022-09-16T10:02:08.583352Z","iopub.status.idle":"2022-09-16T10:02:09.380612Z","shell.execute_reply.started":"2022-09-16T10:02:08.583313Z","shell.execute_reply":"2022-09-16T10:02:09.379690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\n\ni=5\nj=i+1\n\nX = np.array(X_test_raw.iloc[i*672:j*672][['CB_First_Floor', 'CB_Second_Floor']])\nanomaly_true = X_test_raw.iloc[i*672:j*672]['Anomaly_Label_GT'].copy()\nanomaly_prediction = X_test_raw.iloc[i*672:j*672]['Anomaly_Predicted'].copy()\nreconstruction_probability = X_test_raw.iloc[i*672:j*672]['Reconstruction_Probability'].copy()\n\n# Plotting predictionsreconstruction_loss2 = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, y), axis=1))\ninspect_multivariate_prediction(X, X, anomaly_true, anomaly_prediction, reconstruction_probability, target_labels)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:02:15.495824Z","iopub.execute_input":"2022-09-16T10:02:15.497051Z","iopub.status.idle":"2022-09-16T10:02:16.706471Z","shell.execute_reply.started":"2022-09-16T10:02:15.497000Z","shell.execute_reply":"2022-09-16T10:02:16.705597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\n\ni=6\nj=i+1\n\nX = np.array(X_test_raw.iloc[i*672:j*672][['CB_First_Floor', 'CB_Second_Floor']])\nanomaly_true = X_test_raw.iloc[i*672:j*672]['Anomaly_Label_GT'].copy()\nanomaly_prediction = X_test_raw.iloc[i*672:j*672]['Anomaly_Predicted'].copy()\nreconstruction_probability = X_test_raw.iloc[i*672:j*672]['Reconstruction_Probability'].copy()\n\n# Plotting predictionsreconstruction_loss2 = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, y), axis=1))\ninspect_multivariate_prediction(X, X, anomaly_true, anomaly_prediction, reconstruction_probability, target_labels)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:02:19.872843Z","iopub.execute_input":"2022-09-16T10:02:19.873297Z","iopub.status.idle":"2022-09-16T10:02:20.726180Z","shell.execute_reply.started":"2022-09-16T10:02:19.873257Z","shell.execute_reply":"2022-09-16T10:02:20.723755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\n\ni=7\nj=i+1\n\nX = np.array(X_test_raw.iloc[i*672:j*672][['CB_First_Floor', 'CB_Second_Floor']])\nanomaly_true = X_test_raw.iloc[i*672:j*672]['Anomaly_Label_GT'].copy()\nanomaly_prediction = X_test_raw.iloc[i*672:j*672]['Anomaly_Predicted'].copy()\nreconstruction_probability = X_test_raw.iloc[i*672:j*672]['Reconstruction_Probability'].copy()\n\n# Plotting predictionsreconstruction_loss2 = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, y), axis=1))\ninspect_multivariate_prediction(X, X, anomaly_true, anomaly_prediction, reconstruction_probability, target_labels)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:02:23.586841Z","iopub.execute_input":"2022-09-16T10:02:23.587202Z","iopub.status.idle":"2022-09-16T10:02:24.180145Z","shell.execute_reply.started":"2022-09-16T10:02:23.587170Z","shell.execute_reply":"2022-09-16T10:02:24.178685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\n\ni=8\nj=i+1\n\nX = np.array(X_test_raw.iloc[i*672:j*672][['CB_First_Floor', 'CB_Second_Floor']])\nanomaly_true = X_test_raw.iloc[i*672:j*672]['Anomaly_Label_GT'].copy()\nanomaly_prediction = X_test_raw.iloc[i*672:j*672]['Anomaly_Predicted'].copy()\nreconstruction_probability = X_test_raw.iloc[i*672:j*672]['Reconstruction_Probability'].copy()\n\n# Plotting predictionsreconstruction_loss2 = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, y), axis=1))\ninspect_multivariate_prediction(X, X, anomaly_true, anomaly_prediction, reconstruction_probability, target_labels)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:02:28.328776Z","iopub.execute_input":"2022-09-16T10:02:28.329538Z","iopub.status.idle":"2022-09-16T10:02:28.970886Z","shell.execute_reply.started":"2022-09-16T10:02:28.329485Z","shell.execute_reply":"2022-09-16T10:02:28.969610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\n\ni=9\nj=i+1\n\nX = np.array(X_test_raw.iloc[i*672:j*672][['CB_First_Floor', 'CB_Second_Floor']])\nanomaly_true = X_test_raw.iloc[i*672:j*672]['Anomaly_Label_GT'].copy()\nanomaly_prediction = X_test_raw.iloc[i*672:j*672]['Anomaly_Predicted'].copy()\nreconstruction_probability = X_test_raw.iloc[i*672:j*672]['Reconstruction_Probability'].copy()\n\n# Plotting predictionsreconstruction_loss2 = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, y), axis=1))\ninspect_multivariate_prediction(X, X, anomaly_true, anomaly_prediction, reconstruction_probability, target_labels)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:02:33.385488Z","iopub.execute_input":"2022-09-16T10:02:33.386026Z","iopub.status.idle":"2022-09-16T10:02:34.019693Z","shell.execute_reply.started":"2022-09-16T10:02:33.385993Z","shell.execute_reply":"2022-09-16T10:02:34.018587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\n\ni=10\nj=i+1\n\nX = np.array(X_test_raw.iloc[i*672:j*672][['CB_First_Floor', 'CB_Second_Floor']])\nanomaly_true = X_test_raw.iloc[i*672:j*672]['Anomaly_Label_GT'].copy()\nanomaly_prediction = X_test_raw.iloc[i*672:j*672]['Anomaly_Predicted'].copy()\nreconstruction_probability = X_test_raw.iloc[i*672:j*672]['Reconstruction_Probability'].copy()\n\n# Plotting predictionsreconstruction_loss2 = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, y), axis=1))\ninspect_multivariate_prediction(X, X, anomaly_true, anomaly_prediction, reconstruction_probability, target_labels)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:02:38.136437Z","iopub.execute_input":"2022-09-16T10:02:38.136934Z","iopub.status.idle":"2022-09-16T10:02:38.813776Z","shell.execute_reply.started":"2022-09-16T10:02:38.136892Z","shell.execute_reply":"2022-09-16T10:02:38.812586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anomaly_predicted = anomaly_predicted\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_true=Anomaly_Test, y_pred=anomaly_predicted)\n\n# Compute the classification metrics\naccuracy = accuracy_score(y_true=Anomaly_Test, y_pred=anomaly_predicted)\nprecision = precision_score(y_true=Anomaly_Test, y_pred=anomaly_predicted)\nrecall = recall_score(y_true=Anomaly_Test, y_pred=anomaly_predicted)\nf1 = f1_score(y_true=Anomaly_Test, y_pred=anomaly_predicted)\nprint('Accuracy:',accuracy.round(4))\nprint('Precision:',precision.round(4))\nprint('Recall:',recall.round(4))\nprint('F1:',f1.round(4))\n\n# Plot the confusion matrix\nfig, ax = plt.subplots(figsize=(5, 5))\nax.matshow(cm, cmap=plt.cm.Oranges, alpha=0.3)\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n        \nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T10:02:40.957459Z","iopub.execute_input":"2022-09-16T10:02:40.957943Z","iopub.status.idle":"2022-09-16T10:02:41.429064Z","shell.execute_reply.started":"2022-09-16T10:02:40.957899Z","shell.execute_reply":"2022-09-16T10:02:41.422567Z"},"trusted":true},"execution_count":null,"outputs":[]}]}